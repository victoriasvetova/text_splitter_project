
# Text Splitter — восстановление пропущенных пробелов в тексте

Этот проект решает задачу восстановления пропущенных пробелов в тексте на русском языке.  
Он автоматически разбивает слитные строки на отдельные слова с помощью морфологического анализатора [`pymorphy3`](https://pymorphy3.readthedocs.io/).

Пример:
```

Вход:  куплютелевизорPhilips
Выход: куплю телевизор Philips

```

---

## Постановка задачи

На Авито пользователи часто вводят тексты с опечатками, слитным написанием или пропущенными пробелами:

```

книгавхорошемсостоянии
телефонсамсунгбезцарапин
купитьайфон15спб

```

Такие тексты:
- ухудшают качество поиска,
- мешают рекомендательным алгоритмам,
- осложняют дальнейшие NLP-задачи (извлечение сущностей, классификация и т.д.).

Цель: разработать лёгкий, быстрый и воспроизводимый алгоритм, который принимает на вход строку без пробелов и возвращает строку с восстановленными пробелами, а также позиции вставок.


---

## Как решена задача в проекте

Алгоритм сводит задачу к поиску оптимальной сегментации строки на слова.

1. Морфологический анализ (pymorphy3)

- Для каждого фрагмента текста вызывается morph.parse(), возвращающая возможные разборы.

- Используем только валидные (словарные) разборы.

- Складываем их score, получая «правдоподобность» слова.

2. Функция качества сегментации

Общий скор сегментации — сумма скор всех слов. Чем выше сумма, тем более «грамотно» выглядит разбиение.

3. Поиск

Перебираем все возможные разрезы строки. Для каждого конца слова считаем скор и продолжаем рекурсивно.

Используется жадный поиск: сначала проверяются фрагменты с наибольшим скором.

Такое решение быстро работает на CPU, не требует обучения и эффективно справляется с пользовательскими строками.

---

## Структура проекта

```

text_splitter_project/
├── text_splitter/             # Основной код
│   ├── __init__.py
│   ├── splitter.py            # split_text, calc_score
│   └── utils.py               # get_space_positions
├── data/                      # Входные файлы (сюда клади свой датасет)
│   └── dataset_1937770_3.txt  # Пример входного файла
├── output/                    # Результаты обработки
├── scripts/
│   └── run_splitter.py        # Скрипт запуска
├── requirements.txt
├── README.md

````

## Описание кода

### text_splitter/splitter.py
Содержит алгоритм восстановления пробелов:
- `calc_score(z)` — получает список морфологических разборов (`pymorphy3.parse`) и считает "надежность" фрагмента. Отбрасывает неизвестные и искусственные разборы.
- `split_text(text, start=0)` — рекурсивно разбивает строку без пробелов на токены (слова). Возвращает список слов. Использует `calc_score` для выбора наилучших кандидатов.

### text_splitter/utils.py
Вспомогательные функции:
- `get_space_positions(original, split_tokens)` — вычисляет позиции, где должны стоять пробелы, по исходной строке без пробелов. Например:  
  `"книгавхорошемсостоянии"` → `["книга", "в", "хорошем", "состоянии"]` → `[5, 6, 13]`.

### scripts/run_splitter.py
Скрипт для запуска пайплайна:
1. Загружает датасет (`data/dataset_1937770_3.txt` или любой другой CSV).  
2. Для каждой строки вызывает `split_text`.  
3. Получает позиции пробелов через `get_space_positions`.  
4. Сохраняет результат в CSV (`output/predicted_positions.csv`).



---

## Быстрый старт

### 1. Клонировать репозиторий

```bash
git clone https://github.com/your_username/text_splitter_project.git
cd text_splitter_project
````

### 2. Создать и активировать виртуальное окружение

```bash
python -m venv nlp_task_env
```

#### Windows (cmd):

```cmd
nlp_task_env\Scripts\activate
```

#### Windows (PowerShell):

```powershell
nlp_task_env\Scripts\Activate.ps1
```

#### Linux / macOS:

```bash
source nlp_task_env/bin/activate
```

### 3. Установить зависимости

```bash
pip install -r requirements.txt
```

---

## Входной файл

CSV-файл с двумя колонками и заголовком:

| id | text_no_spaces      |
| -- | --------------------- |
| 1  | куплютелевизорPhilips |

Файл нужно положить в папку `data/`.
Пример: `data/my_dataset.csv`

---

## 4. Запуск

### Запуск по умолчанию (использует встроенный файл):

```bash
python scripts/run_splitter.py
```

### Запуск со своим датасетом:

```bash
python scripts/run_splitter.py data/dataset_1937770_3.txt output/my_result.csv
```

* `data/dataset_1937770_3.txt` — путь к вашему файлу
* `output/my_result.csv` — файл с результатами (создастся автоматически)

---

## Выходной результат

CSV-файл:

| id | predicted_positions |
| -- | -------------------- |
| 1  | [5, 15]             |

Это означает: в строке без пробелов пробелы вставляются после символов с индексами 5 и 15.

---



---

## Ограничения и возможные улучшения

* Не всегда корректно работает → можно добавить кастомный словарь.
* Не учитывает длинные синтаксические зависимости → можно подключить n-граммы или языковую модель.
* Для масштабного решения (поиск, реклама) можно комбинировать морфологию с лёгкой нейросетью (BiLSTM, CRF).

---

## Обратная связь

Буду рада замечаниям.



